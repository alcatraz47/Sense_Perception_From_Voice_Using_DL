## Emotion Recognition from Voice Using Deep Learning

### Objective:

Detect human emotions from a single word and to add context from the same keywords "say the word" with it. This research is performed using the open dataset named T-space of the University of Toronto. Two female actors performed the voice over. The key objective of this study was to detect whether a deep neural network can or cannot detect the intensity of the emotion in the human voice. The testing stage of this research was also later performed on a noisy environment with some male actor voice to see the adaptability of the models that were generated by using T-space dataset.

**Note**: This research was performed using the dataset which was generated in an artificial environment by the act of the actors. The stimulating environment might not be appropriate for the real world. So, the researchers and authors of this research do not suggest to use it in the real world.

### Dataset:

The dataset contained seven classes and they are Sad, Angry, Happy, Pleasant Surprise, Fear, Neutral, and Disgust. The actors are of two different ages. One is a young female, and another one is an old female voice. The voice-over was taken in a restricted and non-noisy environment. Each of the age group contains 200 voice clips for each class; altogether 1400 voice clips for each class.

### Dataset Pre-processing:

Speech processing was used on the dataset in interest using MFCC feature extraction and later on, Mel Spectrogram was used on them to deduct the features.

The audio files were in wave format, and after constructing into Fast Fourier Transformation, the spectrogram of that FFT(Fast Fourier Transformation) were designed to extract the features. From spectrogram, Mel filter was used on those data. These features are used in our further experiment. In figure 1, we have demonstrated what actual voice data and the MEL feature of that voice data looks like. In this figure, the first row shows the plot of actual voice data in visual form and in the second row shows the plots of MEL features while is converted from the actual voice data. After that, those features are normalized using mean normalization to produce a common scale for the dataset. 

![Data Set Visualization](https://github.com/alcatraz47/Sense_Perception_From_Voice_Using_DL/blob/master/images/mel-feature.png)

During the extraction of the features, we used a sampling rate of 16000. Those extracted feature vectors are different in size as those clips are not in the same size. The clips size varies from 1500 to 2300 milliseconds. In this case, first, we calculated the mean length for each clip feature vector to get average data size. Depending on this, we kept the maximum length that can be used by us. The smaller clips, which was less than the mean value, was filled out with zeros to get the same length. This technique is done for all kinds of data we used in this work.

### Data Partitioning:

We used different types of data partition to evaluate our models. First, we tried with the 80/20 split; 80% for training purpose and 20% for testing purpose. But it was not giving us the expected result which was far away from our result. After that, we picked the validation data from different intervals like; picking one clip after one, five or ten. The intension behind this approach is to get all kinds of class data equally as the common types of data partitioning is not working here. Table 2 shows the dataset partitioning which used to trained and evaluated individualâ€™s class for the different age group. We also trained our models by merging two different age groups as each age group has same type of emotion data. As we have merged them, now there are 7 types of emotion class.

### Best Model:

![Best CNN Model](https://github.com/alcatraz47/Sense_Perception_From_Voice_Using_DL/blob/master/images/cnn.png)

![Best LSTM Model](https://github.com/alcatraz47/Sense_Perception_From_Voice_Using_DL/blob/master/images/lstm.png)

### Result Analysis:

![Result From CNN](https://github.com/alcatraz47/Sense_Perception_From_Voice_Using_DL/blob/master/images/result_cnn.png)

![Result from LSTM](https://github.com/alcatraz47/Sense_Perception_From_Voice_Using_DL/blob/master/images/result_lstm.png)

![Confusion Matrix](https://github.com/alcatraz47/Sense_Perception_From_Voice_Using_DL/blob/master/images/confusion_matrix.png)

### Conclusion

After that, we created a test by our self to see how it responds to completely new and seen data. While we are creating the data, we kept track of how the actual data is produced like the pitch or time taken for each clip. However, the test amount is much less than the original dataset. We used 30 clips of those kinds of data. We got the highest score of 41.18%. There is several factors for this score, like the test data consist only voices from men but all the actual data is produced by women. As the MEL feature is not similar between them, it can be a reason. 

### For contact:

Md. Mahmudul Haque: mahmudulhaquearfan@gmail.com
Sharif Ahamed: sharifahamed485@gmail.com
